{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8180953,"sourceType":"datasetVersion","datasetId":4843464},{"sourceId":8182367,"sourceType":"datasetVersion","datasetId":4844532}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T05:49:46.276736Z","iopub.execute_input":"2024-05-05T05:49:46.277083Z","iopub.status.idle":"2024-05-05T05:49:46.281623Z","shell.execute_reply.started":"2024-05-05T05:49:46.277053Z","shell.execute_reply":"2024-05-05T05:49:46.280531Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import json","metadata":{"execution":{"iopub.status.busy":"2024-05-05T05:49:51.826905Z","iopub.execute_input":"2024-05-05T05:49:51.827912Z","iopub.status.idle":"2024-05-05T05:49:51.832808Z","shell.execute_reply.started":"2024-05-05T05:49:51.827865Z","shell.execute_reply":"2024-05-05T05:49:51.831560Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/my-own-dataset/data.json', 'r') as file:\n    data = json.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T05:49:54.876676Z","iopub.execute_input":"2024-05-05T05:49:54.877409Z","iopub.status.idle":"2024-05-05T05:49:57.309858Z","shell.execute_reply.started":"2024-05-05T05:49:54.877378Z","shell.execute_reply":"2024-05-05T05:49:57.309071Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\nfrom datasets import Dataset\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/working/fine_tuned_model\")\ntokenizer = AutoTokenizer.from_pretrained(\"MIIB-NLP/Arabic-question-generation\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T06:56:02.151599Z","iopub.execute_input":"2024-05-05T06:56:02.152494Z","iopub.status.idle":"2024-05-05T06:56:04.326408Z","shell.execute_reply.started":"2024-05-05T06:56:02.152459Z","shell.execute_reply":"2024-05-05T06:56:04.325300Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"dataset = Dataset.from_list(data)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T05:57:52.903125Z","iopub.execute_input":"2024-05-05T05:57:52.903536Z","iopub.status.idle":"2024-05-05T05:57:53.445611Z","shell.execute_reply.started":"2024-05-05T05:57:52.903506Z","shell.execute_reply":"2024-05-05T05:57:53.444713Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.shuffle(seed=42).select(range(10000))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T05:57:55.742665Z","iopub.execute_input":"2024-05-05T05:57:55.742999Z","iopub.status.idle":"2024-05-05T05:57:55.772167Z","shell.execute_reply.started":"2024-05-05T05:57:55.742976Z","shell.execute_reply":"2024-05-05T05:57:55.771190Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"def tokenize(example):\n    input_text = \"context: \" + example['context'][0] + \" answer: \" + example['answer'][0] + \" </s>\"\n    target_text = example['question']\n    \n    # Tokenize input text\n    input_encoding = tokenizer(input_text, padding='max_length', truncation=True, max_length=512)\n    input_ids = input_encoding['input_ids']\n    attention_mask = input_encoding['attention_mask']\n    \n    # Tokenize target text\n    target_encoding = tokenizer(target_text, padding='max_length', truncation=True, max_length=512)\n    target_ids = target_encoding['input_ids']\n    \n    # Prepare decoder input IDs\n    # For auto-regressive models, like GPT, the decoder input is the target sequence shifted by one\n    decoder_input_ids = target_ids[:-1]  # Remove last token, as it should not be fed into the model\n    \n    # Return the tokenized inputs as a dictionary\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'decoder_input_ids': decoder_input_ids,  # Specify decoder input IDs\n        'labels': target_ids[1:],  # Shifted target IDs for computing loss\n    }\n\n# Tokenize the dataset\ndataset = dataset.map(tokenize, batch_size=len(dataset))","metadata":{"execution":{"iopub.status.busy":"2024-05-05T05:57:59.063176Z","iopub.execute_input":"2024-05-05T05:57:59.063876Z","iopub.status.idle":"2024-05-05T05:58:10.013902Z","shell.execute_reply.started":"2024-05-05T05:57:59.063845Z","shell.execute_reply":"2024-05-05T05:58:10.012943Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c597c1ffc7504b54852bbbc6f2b0e4ab"}},"metadata":{}}]},{"cell_type":"code","source":"# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=1,              # total number of training epochs\n    per_device_train_batch_size=32,  # batch size per device during training\n    per_device_eval_batch_size=32,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    save_strategy=\"no\" \n)\n\n\n\n# Create the Trainer and train\ntrainer = Trainer(\n    model=model,                         # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=dataset,               # training dataset\n)\n\ntrainer.train()\n\n# Save the model\n# model.save_pretrained('./fine_tuned_model')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T06:55:11.782574Z","iopub.status.idle":"2024-05-05T06:55:11.782927Z","shell.execute_reply.started":"2024-05-05T06:55:11.782758Z","shell.execute_reply":"2024-05-05T06:55:11.782773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained('./fine_tuned_model')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T06:37:28.087060Z","iopub.execute_input":"2024-05-05T06:37:28.087715Z","iopub.status.idle":"2024-05-05T06:37:29.960577Z","shell.execute_reply.started":"2024-05-05T06:37:28.087675Z","shell.execute_reply":"2024-05-05T06:37:29.959394Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def get_question(context,answer):\n  text=\"context: \" +context + \" \" + \"answer: \" + answer + \" </s>\"\n  text_encoding = tokenizer.encode_plus(\n      text,return_tensors=\"pt\"\n  )\n  model.eval()\n  generated_ids =  model.generate(\n    input_ids=text_encoding['input_ids'],\n    attention_mask=text_encoding['attention_mask'],\n    max_length=64,\n    num_beams=5,\n    num_return_sequences=1\n  )\n  return tokenizer.decode(generated_ids[0],skip_special_tokens=True,clean_up_tokenization_spaces=True).replace('question: ',' ')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T06:56:13.481398Z","iopub.execute_input":"2024-05-05T06:56:13.482244Z","iopub.status.idle":"2024-05-05T06:56:13.492095Z","shell.execute_reply.started":"2024-05-05T06:56:13.482190Z","shell.execute_reply":"2024-05-05T06:56:13.490792Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"context=\"الثورة الجزائرية أو ثورة المليون شهيد، اندلعت في 1 نوفمبر 1954 ضد المستعمر الفرنسي ودامت 7 سنوات ونصف. استشهد فيها أكثر من مليون ونصف مليون جزائري\"\nanswer =\" 7 سنوات\"\n\nget_question(context,answer)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T07:04:09.381965Z","iopub.execute_input":"2024-05-05T07:04:09.382726Z","iopub.status.idle":"2024-05-05T07:04:16.162544Z","shell.execute_reply.started":"2024-05-05T07:04:09.382691Z","shell.execute_reply":"2024-05-05T07:04:16.161458Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"''"},"metadata":{}}]},{"cell_type":"code","source":"model.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-05-05T06:53:52.868391Z","iopub.execute_input":"2024-05-05T06:53:52.868765Z","iopub.status.idle":"2024-05-05T06:53:52.889024Z","shell.execute_reply.started":"2024-05-05T06:53:52.868736Z","shell.execute_reply":"2024-05-05T06:53:52.888114Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(110080, 768)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(110080, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(110080, 768)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=110080, bias=False)\n)"},"metadata":{}}]}]}